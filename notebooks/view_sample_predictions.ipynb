{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a43587bf-8bc6-46ce-9345-fc1691e99498",
   "metadata": {},
   "source": [
    "# View sample predictions\n",
    "\n",
    "Trained model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77d35f0-47b5-4f9c-bcb0-002049e5fdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/miniconda3/envs/swe-parl-mot-summarization/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/erik/miniconda3/envs/swe-parl-mot-summarization/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from transformers import MT5Tokenizer\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('..')  # Allow import of project packages\n",
    "from text_summarizer.data.motions_data_module import SweParliamentMotionsDataModule\n",
    "from text_summarizer.models import t5\n",
    "from text_summarizer.lit_models import t5LitModel\n",
    "from text_summarizer.util import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b850804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 618 rows with missing values.\n",
      "Filtered 7442 texts shorter than 150 characters.\n",
      "Number of rows remaining: 159722\n",
      "<text_summarizer.data.motions_data_module.SweParliamentMotionsDataModule object at 0x7f86e4479df0>\n"
     ]
    }
   ],
   "source": [
    "dataset = SweParliamentMotionsDataModule(args=argparse.Namespace())\n",
    "dataset.prepare_data()\n",
    "dataset.setup()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fcdfd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    " \n",
    "MODEL_PATH = \"../training/logs/lightning_logs/version_16/checkpoints/epoch=002-val_loss=0.000-val_cer=0.000.ckpt\"\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f80921",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1dfec9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for t5LitModel:\n\tMissing key(s) in state_dict: \"model.model.encoder.block.0.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.0.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.1.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.1.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.2.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.2.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.3.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.3.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.4.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.4.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.5.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.5.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.6.layer.0.SelfAttention.q.weight\", \"model.model.encoder.block.6.layer.0.SelfAttention.k.weight\", \"model.model.encoder.block.6.layer.0.SelfAttention.v.weight\", \"model.model.encoder.block.6.layer.0.SelfAttention.o.weight\", \"model.model.encoder.block.6.layer.0.layer_norm.weight\", \"model.model.encoder.block.6.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.6.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.6.layer.1.DenseReluDense.wo.weight\", \"model.model.encoder.block.6.layer.1.layer_norm.weight\", \"model.model.encoder.block.7.layer.0.SelfAttention.q.weight\", \"model.model.encoder.block.7.layer.0.SelfAttention.k.weight\", \"model.model.encoder.block.7.layer.0.SelfAttention.v.weight\", \"model.model.encoder.block.7.layer.0.SelfAttention.o.weight\", \"model.model.encoder.block.7.layer.0.layer_norm.weight\", \"model.model.encoder.block.7.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.7.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.7.layer.1.DenseReluDense.wo.weight\", \"model.model.encoder.block.7.layer.1.layer_norm.weight\", \"model.model.encoder.block.8.layer.0.SelfAttention.q.weight\", \"model.model.encoder.block.8.layer.0.SelfAttention.k.weight\", \"model.model.encoder.block.8.layer.0.SelfAttention.v.weight\", \"model.model.encoder.block.8.layer.0.SelfAttention.o.weight\", \"model.model.encoder.block.8.layer.0.layer_norm.weight\", \"model.model.encoder.block.8.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.8.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.8.layer.1.DenseReluDense.wo.weight\", \"model.model.encoder.block.8.layer.1.layer_norm.weight\", \"model.model.encoder.block.9.layer.0.SelfAttention.q.weight\", \"model.model.encoder.block.9.layer.0.SelfAttention.k.weight\", \"model.model.encoder.block.9.layer.0.SelfAttention.v.weight\", \"model.model.encoder.block.9.layer.0.SelfAttention.o.weight\", \"model.model.encoder.block.9.layer.0.layer_norm.weight\", \"model.model.encoder.block.9.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.9.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.9.layer.1.DenseReluDense.wo.weight\", \"model.model.encoder.block.9.layer.1.layer_norm.weight\", \"model.model.encoder.block.10.layer.0.SelfAttention.q.weight\", \"model.model.encoder.block.10.layer.0.SelfAttention.k.weight\", \"model.model.encoder.block.10.layer.0.SelfAttention.v.weight\", \"model.model.encoder.block.10.layer.0.SelfAttention.o.weight\", \"model.model.encoder.block.10.layer.0.layer_norm.weight\", \"model.model.encoder.block.10.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.10.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.10.layer.1.DenseReluDense.wo.weight\", \"model.model.encoder.block.10.layer.1.layer_norm.weight\", \"model.model.encoder.block.11.layer.0.SelfAttention.q.weight\", \"model.model.encoder.block.11.layer.0.SelfAttention.k.weight\", \"model.model.encoder.block.11.layer.0.SelfAttention.v.weight\", \"model.model.encoder.block.11.layer.0.SelfAttention.o.weight\", \"model.model.encoder.block.11.layer.0.layer_norm.weight\", \"model.model.encoder.block.11.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.11.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.11.layer.1.DenseReluDense.wo.weight\", \"model.model.encoder.block.11.layer.1.layer_norm.weight\", \"model.model.decoder.block.0.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.0.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.1.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.1.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.2.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.2.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.3.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.3.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.4.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.4.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.5.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.5.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.6.layer.0.SelfAttention.q.weight\", \"model.model.decoder.block.6.layer.0.SelfAttention.k.weight\", \"model.model.decoder.block.6.layer.0.SelfAttention.v.weight\", \"model.model.decoder.block.6.layer.0.SelfAttention.o.weight\", \"model.model.decoder.block.6.layer.0.layer_norm.weight\", \"model.model.decoder.block.6.layer.1.EncDecAttention.q.weight\", \"model.model.decoder.block.6.layer.1.EncDecAttention.k.weight\", \"model.model.decoder.block.6.layer.1.EncDecAttention.v.weight\", \"model.model.decoder.block.6.layer.1.EncDecAttention.o.weight\", \"model.model.decoder.block.6.layer.1.layer_norm.weight\", \"model.model.decoder.block.6.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.6.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.6.layer.2.DenseReluDense.wo.weight\", \"model.model.decoder.block.6.layer.2.layer_norm.weight\", \"model.model.decoder.block.7.layer.0.SelfAttention.q.weight\", \"model.model.decoder.block.7.layer.0.SelfAttention.k.weight\", \"model.model.decoder.block.7.layer.0.SelfAttention.v.weight\", \"model.model.decoder.block.7.layer.0.SelfAttention.o.weight\", \"model.model.decoder.block.7.layer.0.layer_norm.weight\", \"model.model.decoder.block.7.layer.1.EncDecAttention.q.weight\", \"model.model.decoder.block.7.layer.1.EncDecAttention.k.weight\", \"model.model.decoder.block.7.layer.1.EncDecAttention.v.weight\", \"model.model.decoder.block.7.layer.1.EncDecAttention.o.weight\", \"model.model.decoder.block.7.layer.1.layer_norm.weight\", \"model.model.decoder.block.7.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.7.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.7.layer.2.DenseReluDense.wo.weight\", \"model.model.decoder.block.7.layer.2.layer_norm.weight\", \"model.model.decoder.block.8.layer.0.SelfAttention.q.weight\", \"model.model.decoder.block.8.layer.0.SelfAttention.k.weight\", \"model.model.decoder.block.8.layer.0.SelfAttention.v.weight\", \"model.model.decoder.block.8.layer.0.SelfAttention.o.weight\", \"model.model.decoder.block.8.layer.0.layer_norm.weight\", \"model.model.decoder.block.8.layer.1.EncDecAttention.q.weight\", \"model.model.decoder.block.8.layer.1.EncDecAttention.k.weight\", \"model.model.decoder.block.8.layer.1.EncDecAttention.v.weight\", \"model.model.decoder.block.8.layer.1.EncDecAttention.o.weight\", \"model.model.decoder.block.8.layer.1.layer_norm.weight\", \"model.model.decoder.block.8.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.8.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.8.layer.2.DenseReluDense.wo.weight\", \"model.model.decoder.block.8.layer.2.layer_norm.weight\", \"model.model.decoder.block.9.layer.0.SelfAttention.q.weight\", \"model.model.decoder.block.9.layer.0.SelfAttention.k.weight\", \"model.model.decoder.block.9.layer.0.SelfAttention.v.weight\", \"model.model.decoder.block.9.layer.0.SelfAttention.o.weight\", \"model.model.decoder.block.9.layer.0.layer_norm.weight\", \"model.model.decoder.block.9.layer.1.EncDecAttention.q.weight\", \"model.model.decoder.block.9.layer.1.EncDecAttention.k.weight\", \"model.model.decoder.block.9.layer.1.EncDecAttention.v.weight\", \"model.model.decoder.block.9.layer.1.EncDecAttention.o.weight\", \"model.model.decoder.block.9.layer.1.layer_norm.weight\", \"model.model.decoder.block.9.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.9.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.9.layer.2.DenseReluDense.wo.weight\", \"model.model.decoder.block.9.layer.2.layer_norm.weight\", \"model.model.decoder.block.10.layer.0.SelfAttention.q.weight\", \"model.model.decoder.block.10.layer.0.SelfAttention.k.weight\", \"model.model.decoder.block.10.layer.0.SelfAttention.v.weight\", \"model.model.decoder.block.10.layer.0.SelfAttention.o.weight\", \"model.model.decoder.block.10.layer.0.layer_norm.weight\", \"model.model.decoder.block.10.layer.1.EncDecAttention.q.weight\", \"model.model.decoder.block.10.layer.1.EncDecAttention.k.weight\", \"model.model.decoder.block.10.layer.1.EncDecAttention.v.weight\", \"model.model.decoder.block.10.layer.1.EncDecAttention.o.weight\", \"model.model.decoder.block.10.layer.1.layer_norm.weight\", \"model.model.decoder.block.10.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.10.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.10.layer.2.DenseReluDense.wo.weight\", \"model.model.decoder.block.10.layer.2.layer_norm.weight\", \"model.model.decoder.block.11.layer.0.SelfAttention.q.weight\", \"model.model.decoder.block.11.layer.0.SelfAttention.k.weight\", \"model.model.decoder.block.11.layer.0.SelfAttention.v.weight\", \"model.model.decoder.block.11.layer.0.SelfAttention.o.weight\", \"model.model.decoder.block.11.layer.0.layer_norm.weight\", \"model.model.decoder.block.11.layer.1.EncDecAttention.q.weight\", \"model.model.decoder.block.11.layer.1.EncDecAttention.k.weight\", \"model.model.decoder.block.11.layer.1.EncDecAttention.v.weight\", \"model.model.decoder.block.11.layer.1.EncDecAttention.o.weight\", \"model.model.decoder.block.11.layer.1.layer_norm.weight\", \"model.model.decoder.block.11.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.11.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.11.layer.2.DenseReluDense.wo.weight\", \"model.model.decoder.block.11.layer.2.layer_norm.weight\". \n\tUnexpected key(s) in state_dict: \"model.model.encoder.block.0.layer.1.DenseReluDense.wi.weight\", \"model.model.encoder.block.1.layer.1.DenseReluDense.wi.weight\", \"model.model.encoder.block.2.layer.1.DenseReluDense.wi.weight\", \"model.model.encoder.block.3.layer.1.DenseReluDense.wi.weight\", \"model.model.encoder.block.4.layer.1.DenseReluDense.wi.weight\", \"model.model.encoder.block.5.layer.1.DenseReluDense.wi.weight\", \"model.model.decoder.block.0.layer.2.DenseReluDense.wi.weight\", \"model.model.decoder.block.1.layer.2.DenseReluDense.wi.weight\", \"model.model.decoder.block.2.layer.2.DenseReluDense.wi.weight\", \"model.model.decoder.block.3.layer.2.DenseReluDense.wi.weight\", \"model.model.decoder.block.4.layer.2.DenseReluDense.wi.weight\", \"model.model.decoder.block.5.layer.2.DenseReluDense.wi.weight\". \n\tsize mismatch for model.model.shared.weight: copying a param with shape torch.Size([32128, 512]) from checkpoint, the shape in current model is torch.Size([250112, 768]).\n\tsize mismatch for model.model.encoder.embed_tokens.weight: copying a param with shape torch.Size([32128, 512]) from checkpoint, the shape in current model is torch.Size([250112, 768]).\n\tsize mismatch for model.model.encoder.block.0.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.0.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.0.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.0.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying a param with shape torch.Size([32, 8]) from checkpoint, the shape in current model is torch.Size([32, 12]).\n\tsize mismatch for model.model.encoder.block.0.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.0.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.encoder.block.0.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.1.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.1.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.1.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.1.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.1.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.1.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.encoder.block.1.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.2.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.2.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.2.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.2.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.2.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.2.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.encoder.block.2.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.3.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.3.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.3.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.3.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.3.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.3.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.encoder.block.3.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.4.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.4.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.4.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.4.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.4.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.4.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.encoder.block.4.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.5.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.5.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.5.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.5.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.5.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.5.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.encoder.block.5.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.final_layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.embed_tokens.weight: copying a param with shape torch.Size([32128, 512]) from checkpoint, the shape in current model is torch.Size([250112, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying a param with shape torch.Size([32, 8]) from checkpoint, the shape in current model is torch.Size([32, 12]).\n\tsize mismatch for model.model.decoder.block.0.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.0.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.0.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.decoder.block.0.layer.2.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.1.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.1.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.1.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.decoder.block.1.layer.2.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.2.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.2.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.2.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.decoder.block.2.layer.2.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.3.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.3.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.3.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.decoder.block.3.layer.2.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.4.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.4.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.4.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.decoder.block.4.layer.2.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.5.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.5.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.5.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.decoder.block.5.layer.2.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.final_layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.lm_head.weight: copying a param with shape torch.Size([32128, 512]) from checkpoint, the shape in current model is torch.Size([250112, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m t5(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     data_config\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mconfig(),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     args\u001b[39m=\u001b[39margparse\u001b[39m.\u001b[39mNamespace()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m lit_model \u001b[39m=\u001b[39m t5LitModel\u001b[39m.\u001b[39;49mload_from_checkpoint(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     checkpoint_path\u001b[39m=\u001b[39;49mMODEL_PATH,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     args\u001b[39m=\u001b[39;49margparse\u001b[39m.\u001b[39;49mNamespace())\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m lit_model\u001b[39m.\u001b[39mfreeze()\n",
      "File \u001b[0;32m~/miniconda3/envs/swe-parl-mot-summarization/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:161\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m# override the hparams with values that were passed in\u001b[39;00m\n\u001b[1;32m    159\u001b[0m checkpoint[\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mCHECKPOINT_HYPER_PARAMS_KEY]\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m--> 161\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_model_state(checkpoint, strict\u001b[39m=\u001b[39;49mstrict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    162\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/swe-parl-mot-summarization/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:209\u001b[0m, in \u001b[0;36mModelIO._load_model_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    206\u001b[0m model\u001b[39m.\u001b[39mon_load_checkpoint(checkpoint)\n\u001b[1;32m    208\u001b[0m \u001b[39m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m keys \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mload_state_dict(checkpoint[\u001b[39m\"\u001b[39;49m\u001b[39mstate_dict\u001b[39;49m\u001b[39m\"\u001b[39;49m], strict\u001b[39m=\u001b[39;49mstrict)\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m strict:\n\u001b[1;32m    212\u001b[0m     \u001b[39mif\u001b[39;00m keys\u001b[39m.\u001b[39mmissing_keys:\n",
      "File \u001b[0;32m~/miniconda3/envs/swe-parl-mot-summarization/lib/python3.9/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1493\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1494\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1496\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1498\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1499\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for t5LitModel:\n\tMissing key(s) in state_dict: \"model.model.encoder.block.0.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.0.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.1.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.1.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.2.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.2.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.3.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.3.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.4.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.4.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.5.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.5.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.6.layer.0.SelfAttention.q.weight\", \"model.model.encoder.block.6.layer.0.SelfAttention.k.weight\", \"model.model.encoder.block.6.layer.0.SelfAttention.v.weight\", \"model.model.encoder.block.6.layer.0.SelfAttention.o.weight\", \"model.model.encoder.block.6.layer.0.layer_norm.weight\", \"model.model.encoder.block.6.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.6.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.6.layer.1.DenseReluDense.wo.weight\", \"model.model.encoder.block.6.layer.1.layer_norm.weight\", \"model.model.encoder.block.7.layer.0.SelfAttention.q.weight\", \"model.model.encoder.block.7.layer.0.SelfAttention.k.weight\", \"model.model.encoder.block.7.layer.0.SelfAttention.v.weight\", \"model.model.encoder.block.7.layer.0.SelfAttention.o.weight\", \"model.model.encoder.block.7.layer.0.layer_norm.weight\", \"model.model.encoder.block.7.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.7.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.7.layer.1.DenseReluDense.wo.weight\", \"model.model.encoder.block.7.layer.1.layer_norm.weight\", \"model.model.encoder.block.8.layer.0.SelfAttention.q.weight\", \"model.model.encoder.block.8.layer.0.SelfAttention.k.weight\", \"model.model.encoder.block.8.layer.0.SelfAttention.v.weight\", \"model.model.encoder.block.8.layer.0.SelfAttention.o.weight\", \"model.model.encoder.block.8.layer.0.layer_norm.weight\", \"model.model.encoder.block.8.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.8.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.8.layer.1.DenseReluDense.wo.weight\", \"model.model.encoder.block.8.layer.1.layer_norm.weight\", \"model.model.encoder.block.9.layer.0.SelfAttention.q.weight\", \"model.model.encoder.block.9.layer.0.SelfAttention.k.weight\", \"model.model.encoder.block.9.layer.0.SelfAttention.v.weight\", \"model.model.encoder.block.9.layer.0.SelfAttention.o.weight\", \"model.model.encoder.block.9.layer.0.layer_norm.weight\", \"model.model.encoder.block.9.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.9.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.9.layer.1.DenseReluDense.wo.weight\", \"model.model.encoder.block.9.layer.1.layer_norm.weight\", \"model.model.encoder.block.10.layer.0.SelfAttention.q.weight\", \"model.model.encoder.block.10.layer.0.SelfAttention.k.weight\", \"model.model.encoder.block.10.layer.0.SelfAttention.v.weight\", \"model.model.encoder.block.10.layer.0.SelfAttention.o.weight\", \"model.model.encoder.block.10.layer.0.layer_norm.weight\", \"model.model.encoder.block.10.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.10.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.10.layer.1.DenseReluDense.wo.weight\", \"model.model.encoder.block.10.layer.1.layer_norm.weight\", \"model.model.encoder.block.11.layer.0.SelfAttention.q.weight\", \"model.model.encoder.block.11.layer.0.SelfAttention.k.weight\", \"model.model.encoder.block.11.layer.0.SelfAttention.v.weight\", \"model.model.encoder.block.11.layer.0.SelfAttention.o.weight\", \"model.model.encoder.block.11.layer.0.layer_norm.weight\", \"model.model.encoder.block.11.layer.1.DenseReluDense.wi_0.weight\", \"model.model.encoder.block.11.layer.1.DenseReluDense.wi_1.weight\", \"model.model.encoder.block.11.layer.1.DenseReluDense.wo.weight\", \"model.model.encoder.block.11.layer.1.layer_norm.weight\", \"model.model.decoder.block.0.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.0.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.1.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.1.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.2.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.2.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.3.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.3.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.4.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.4.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.5.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.5.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.6.layer.0.SelfAttention.q.weight\", \"model.model.decoder.block.6.layer.0.SelfAttention.k.weight\", \"model.model.decoder.block.6.layer.0.SelfAttention.v.weight\", \"model.model.decoder.block.6.layer.0.SelfAttention.o.weight\", \"model.model.decoder.block.6.layer.0.layer_norm.weight\", \"model.model.decoder.block.6.layer.1.EncDecAttention.q.weight\", \"model.model.decoder.block.6.layer.1.EncDecAttention.k.weight\", \"model.model.decoder.block.6.layer.1.EncDecAttention.v.weight\", \"model.model.decoder.block.6.layer.1.EncDecAttention.o.weight\", \"model.model.decoder.block.6.layer.1.layer_norm.weight\", \"model.model.decoder.block.6.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.6.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.6.layer.2.DenseReluDense.wo.weight\", \"model.model.decoder.block.6.layer.2.layer_norm.weight\", \"model.model.decoder.block.7.layer.0.SelfAttention.q.weight\", \"model.model.decoder.block.7.layer.0.SelfAttention.k.weight\", \"model.model.decoder.block.7.layer.0.SelfAttention.v.weight\", \"model.model.decoder.block.7.layer.0.SelfAttention.o.weight\", \"model.model.decoder.block.7.layer.0.layer_norm.weight\", \"model.model.decoder.block.7.layer.1.EncDecAttention.q.weight\", \"model.model.decoder.block.7.layer.1.EncDecAttention.k.weight\", \"model.model.decoder.block.7.layer.1.EncDecAttention.v.weight\", \"model.model.decoder.block.7.layer.1.EncDecAttention.o.weight\", \"model.model.decoder.block.7.layer.1.layer_norm.weight\", \"model.model.decoder.block.7.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.7.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.7.layer.2.DenseReluDense.wo.weight\", \"model.model.decoder.block.7.layer.2.layer_norm.weight\", \"model.model.decoder.block.8.layer.0.SelfAttention.q.weight\", \"model.model.decoder.block.8.layer.0.SelfAttention.k.weight\", \"model.model.decoder.block.8.layer.0.SelfAttention.v.weight\", \"model.model.decoder.block.8.layer.0.SelfAttention.o.weight\", \"model.model.decoder.block.8.layer.0.layer_norm.weight\", \"model.model.decoder.block.8.layer.1.EncDecAttention.q.weight\", \"model.model.decoder.block.8.layer.1.EncDecAttention.k.weight\", \"model.model.decoder.block.8.layer.1.EncDecAttention.v.weight\", \"model.model.decoder.block.8.layer.1.EncDecAttention.o.weight\", \"model.model.decoder.block.8.layer.1.layer_norm.weight\", \"model.model.decoder.block.8.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.8.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.8.layer.2.DenseReluDense.wo.weight\", \"model.model.decoder.block.8.layer.2.layer_norm.weight\", \"model.model.decoder.block.9.layer.0.SelfAttention.q.weight\", \"model.model.decoder.block.9.layer.0.SelfAttention.k.weight\", \"model.model.decoder.block.9.layer.0.SelfAttention.v.weight\", \"model.model.decoder.block.9.layer.0.SelfAttention.o.weight\", \"model.model.decoder.block.9.layer.0.layer_norm.weight\", \"model.model.decoder.block.9.layer.1.EncDecAttention.q.weight\", \"model.model.decoder.block.9.layer.1.EncDecAttention.k.weight\", \"model.model.decoder.block.9.layer.1.EncDecAttention.v.weight\", \"model.model.decoder.block.9.layer.1.EncDecAttention.o.weight\", \"model.model.decoder.block.9.layer.1.layer_norm.weight\", \"model.model.decoder.block.9.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.9.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.9.layer.2.DenseReluDense.wo.weight\", \"model.model.decoder.block.9.layer.2.layer_norm.weight\", \"model.model.decoder.block.10.layer.0.SelfAttention.q.weight\", \"model.model.decoder.block.10.layer.0.SelfAttention.k.weight\", \"model.model.decoder.block.10.layer.0.SelfAttention.v.weight\", \"model.model.decoder.block.10.layer.0.SelfAttention.o.weight\", \"model.model.decoder.block.10.layer.0.layer_norm.weight\", \"model.model.decoder.block.10.layer.1.EncDecAttention.q.weight\", \"model.model.decoder.block.10.layer.1.EncDecAttention.k.weight\", \"model.model.decoder.block.10.layer.1.EncDecAttention.v.weight\", \"model.model.decoder.block.10.layer.1.EncDecAttention.o.weight\", \"model.model.decoder.block.10.layer.1.layer_norm.weight\", \"model.model.decoder.block.10.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.10.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.10.layer.2.DenseReluDense.wo.weight\", \"model.model.decoder.block.10.layer.2.layer_norm.weight\", \"model.model.decoder.block.11.layer.0.SelfAttention.q.weight\", \"model.model.decoder.block.11.layer.0.SelfAttention.k.weight\", \"model.model.decoder.block.11.layer.0.SelfAttention.v.weight\", \"model.model.decoder.block.11.layer.0.SelfAttention.o.weight\", \"model.model.decoder.block.11.layer.0.layer_norm.weight\", \"model.model.decoder.block.11.layer.1.EncDecAttention.q.weight\", \"model.model.decoder.block.11.layer.1.EncDecAttention.k.weight\", \"model.model.decoder.block.11.layer.1.EncDecAttention.v.weight\", \"model.model.decoder.block.11.layer.1.EncDecAttention.o.weight\", \"model.model.decoder.block.11.layer.1.layer_norm.weight\", \"model.model.decoder.block.11.layer.2.DenseReluDense.wi_0.weight\", \"model.model.decoder.block.11.layer.2.DenseReluDense.wi_1.weight\", \"model.model.decoder.block.11.layer.2.DenseReluDense.wo.weight\", \"model.model.decoder.block.11.layer.2.layer_norm.weight\". \n\tUnexpected key(s) in state_dict: \"model.model.encoder.block.0.layer.1.DenseReluDense.wi.weight\", \"model.model.encoder.block.1.layer.1.DenseReluDense.wi.weight\", \"model.model.encoder.block.2.layer.1.DenseReluDense.wi.weight\", \"model.model.encoder.block.3.layer.1.DenseReluDense.wi.weight\", \"model.model.encoder.block.4.layer.1.DenseReluDense.wi.weight\", \"model.model.encoder.block.5.layer.1.DenseReluDense.wi.weight\", \"model.model.decoder.block.0.layer.2.DenseReluDense.wi.weight\", \"model.model.decoder.block.1.layer.2.DenseReluDense.wi.weight\", \"model.model.decoder.block.2.layer.2.DenseReluDense.wi.weight\", \"model.model.decoder.block.3.layer.2.DenseReluDense.wi.weight\", \"model.model.decoder.block.4.layer.2.DenseReluDense.wi.weight\", \"model.model.decoder.block.5.layer.2.DenseReluDense.wi.weight\". \n\tsize mismatch for model.model.shared.weight: copying a param with shape torch.Size([32128, 512]) from checkpoint, the shape in current model is torch.Size([250112, 768]).\n\tsize mismatch for model.model.encoder.embed_tokens.weight: copying a param with shape torch.Size([32128, 512]) from checkpoint, the shape in current model is torch.Size([250112, 768]).\n\tsize mismatch for model.model.encoder.block.0.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.0.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.0.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.0.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying a param with shape torch.Size([32, 8]) from checkpoint, the shape in current model is torch.Size([32, 12]).\n\tsize mismatch for model.model.encoder.block.0.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.0.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.encoder.block.0.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.1.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.1.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.1.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.1.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.1.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.1.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.encoder.block.1.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.2.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.2.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.2.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.2.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.2.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.2.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.encoder.block.2.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.3.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.3.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.3.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.3.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.3.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.3.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.encoder.block.3.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.4.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.4.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.4.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.4.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.4.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.4.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.encoder.block.4.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.5.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.5.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.5.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.5.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.encoder.block.5.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.block.5.layer.1.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.encoder.block.5.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.encoder.final_layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.embed_tokens.weight: copying a param with shape torch.Size([32128, 512]) from checkpoint, the shape in current model is torch.Size([250112, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying a param with shape torch.Size([32, 8]) from checkpoint, the shape in current model is torch.Size([32, 12]).\n\tsize mismatch for model.model.decoder.block.0.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.0.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.0.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.0.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.decoder.block.0.layer.2.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.1.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.1.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.1.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.1.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.decoder.block.1.layer.2.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.2.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.2.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.2.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.2.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.decoder.block.2.layer.2.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.3.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.3.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.3.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.3.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.decoder.block.3.layer.2.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.4.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.4.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.4.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.4.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.decoder.block.4.layer.2.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.5.layer.0.SelfAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.0.SelfAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.0.SelfAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.0.SelfAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.0.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.5.layer.1.EncDecAttention.q.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.1.EncDecAttention.k.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.1.EncDecAttention.v.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.1.EncDecAttention.o.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for model.model.decoder.block.5.layer.1.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.block.5.layer.2.DenseReluDense.wo.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for model.model.decoder.block.5.layer.2.layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.decoder.final_layer_norm.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for model.model.lm_head.weight: copying a param with shape torch.Size([32128, 512]) from checkpoint, the shape in current model is torch.Size([250112, 768])."
     ]
    }
   ],
   "source": [
    "model = t5(\n",
    "    data_config=dataset.config(),\n",
    "    args=argparse.Namespace()\n",
    ")\n",
    "\n",
    "lit_model = t5LitModel.load_from_checkpoint(\n",
    "    checkpoint_path=MODEL_PATH,\n",
    "    model=model,\n",
    "    args=argparse.Namespace())\n",
    "lit_model.freeze()\n",
    "\n",
    "#tokenizer = dataset.data_test.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06018d06",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b09fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(model, text, tokenizer, text_max_num_tokens, summary_max_num_tokens):\n",
    "    text_encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=text_max_num_tokens,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    generated_ids = model.model.generate(\n",
    "        input_ids=text_encoding[\"input_ids\"],\n",
    "        attention_mask=text_encoding[\"attention_mask\"],\n",
    "        max_length=summary_max_num_tokens,\n",
    "        num_beams=2,\n",
    "        repetition_penalty=5.0,\n",
    "        length_penalty=2.0,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    preds = [\n",
    "        tokenizer.decode(\n",
    "            gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        for gen_id in generated_ids\n",
    "    ]\n",
    "\n",
    "    return \"\".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc1df9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_pred():\n",
    "    sample_index = random.randint(0, 100_000)\n",
    "    text = dataset.data_test.data[sample_index]\n",
    "    true_summary = dataset.data_test.targets[sample_index]\n",
    "    model_summary = summarize(\n",
    "        model=model,\n",
    "        text=text,\n",
    "        tokenizer=tokenizer,\n",
    "        text_max_num_tokens=512,\n",
    "        summary_max_num_tokens=64\n",
    "    )\n",
    "    print(\"Motion text:\")\n",
    "    print(50*\"-\")\n",
    "    print(text[:500])\n",
    "    print(50*\"-\")\n",
    "    print(\"Actual title:\")\n",
    "    print(true_summary)\n",
    "    print(50*\"-\")\n",
    "    print(\"Predicted title:\")\n",
    "    print(model_summary)\n",
    "    print(50*\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54b88293",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#ch0000010vscode-remote?line=0'>1</a>\u001b[0m show_sample_pred()\n",
      "\u001b[1;32m/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb Cell 10\u001b[0m in \u001b[0;36mshow_sample_pred\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#ch0000010vscode-remote?line=2'>3</a>\u001b[0m text \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mdata_test\u001b[39m.\u001b[39mdata[sample_index]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#ch0000010vscode-remote?line=3'>4</a>\u001b[0m true_summary \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mdata_test\u001b[39m.\u001b[39mtargets[sample_index]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#ch0000010vscode-remote?line=4'>5</a>\u001b[0m model_summary \u001b[39m=\u001b[39m summarize(\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#ch0000010vscode-remote?line=5'>6</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#ch0000010vscode-remote?line=6'>7</a>\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#ch0000010vscode-remote?line=7'>8</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#ch0000010vscode-remote?line=8'>9</a>\u001b[0m     text_max_num_tokens\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#ch0000010vscode-remote?line=9'>10</a>\u001b[0m     summary_max_num_tokens\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#ch0000010vscode-remote?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#ch0000010vscode-remote?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMotion text:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/erik/proj/swedish_parliament_motion_summarization/notebooks/view_sample_predictions.ipynb#ch0000010vscode-remote?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m50\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "show_sample_pred()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42136c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8183e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = t5(\n",
    "    data_config=dataset.config(),\n",
    "    args=argparse.Namespace()\n",
    ")\n",
    "\n",
    "lit_model = t5LitModel(\n",
    "    model=model,\n",
    "    args=argparse.Namespace())\n",
    "lit_model.freeze()\n",
    "\n",
    "tokenizer = dataset.data_test.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a135f864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion text:\n",
      "--------------------------------------------------\n",
      "Stort behov av fler medicinskt sakkunniga i fibromyalgiFibromyalgi drabbar alltfler och allt längre ner i åldrarna. Debut i 30- till 40- årsåldern blir allt vanligare. Även ännu yngre kan insjukna. Det är fortfarande mest kvinnor som drabbas av fibromyalgi. Man beräknar att närmare 3,5 procent av alla kvinnor har sjukdomen. Dessa unga eller medelålders kvinnor är mycket angelägna att kunna fortsätta att arbeta.Det är därför viktigt att de får en snabb diagnos, lämplig behandling och goda möjligh\n",
      "--------------------------------------------------\n",
      "Actual title:\n",
      "Sakkunniga i fibromyalgi\n",
      "--------------------------------------------------\n",
      "Predicted title:\n",
      "<extra_id_0> i vårt land. Stockholm den\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "show_sample_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb91cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e0eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('swe-parl-mot-summarization')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8b4684d929b0e5f65a062924196a80e8ca5b6afcc06053ab7d36a264e586290"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
